{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic comment classification with deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TED--e84y9M",
        "outputId": "15ca292f-cde5-4a8c-9d7d-433058ad9caf"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgac82G97srU",
        "outputId": "215a3cba-b0f9-4c31-df27-8380b1664a7b"
      },
      "source": [
        "pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOxydcm37st-",
        "outputId": "4df34c9d-ead1-48f4-a92c-98b39e1b2968"
      },
      "source": [
        "import sys  \r\n",
        "!{sys.executable} -m pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/ad/d1c685967945a04f8596128b15a1ab56c51488f53312e953341af6ff22d1/contractions-0.0.43-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 15.1MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 21.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81696 sha256=8453e97323e4af00f3a2e47392888361122516035aa4ebd42869a7e246438062\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "tVgnJ8dGjHMo",
        "outputId": "c98fcf63-530e-4797-a3b8-f26aff873cac"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv_zXY5C7sxG"
      },
      "source": [
        "import pandas as pd\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\r\n",
        "import re\r\n",
        "import contractions\r\n",
        "import pickle\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBDNdRRX7szU"
      },
      "source": [
        "def text_preprocess(sent):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    stemmer = PorterStemmer() \r\n",
        "    \r\n",
        "    # convert all words to lowercase\r\n",
        "    sent = sent.lower()\r\n",
        "    \r\n",
        "    # expand contractions\r\n",
        "    expanded_words = []\r\n",
        "    for word in sent.split():\r\n",
        "        expanded_words.append(contractions.fix(word))\r\n",
        "    sent = ' '.join(expanded_words) \r\n",
        "    \r\n",
        "    # remove html tags\r\n",
        "    sent = re.sub('{html}', \"\", sent)\r\n",
        "    \r\n",
        "    # remove http links and web site url\r\n",
        "    sent = re.sub(r\"http\\S+\", \"\", sent)\r\n",
        "    sent = re.sub(r\"www\\S+\", \"\", sent)\r\n",
        "    \r\n",
        "    # remove numbers\r\n",
        "    sent = re.sub('[0-9]+', '', sent)\r\n",
        "    \r\n",
        "    # remove words utc and image\r\n",
        "    sent = sent.replace('utc', '')\r\n",
        "    \r\n",
        "    # remove words with .jpg\r\n",
        "    sent = re.sub(r\"[a-zA-Z]*[0-9]*.jpg\", '', sent)\r\n",
        "    \r\n",
        "    # remove emails\r\n",
        "    sent = re.sub(r\"\\S*@\\S*\\s?\", '', sent)\r\n",
        "    \r\n",
        "    # tokenization\r\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    tokens = tokenizer.tokenize(sent)\r\n",
        "    \r\n",
        "    # remove stop words\r\n",
        "    filtered_words = [w for w in tokens if w not in stopwords.words('english')]\r\n",
        "    \r\n",
        "    stem_words = [stemmer.stem(w) for w in filtered_words]\r\n",
        "    lemma_words = [lemmatizer.lemmatize(w) for w in stem_words]\r\n",
        "    \r\n",
        "    return \" \".join(lemma_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOT269qb5a8c"
      },
      "source": [
        "text = pd.read_csv('train.csv')\r\n",
        "text.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iscofe0j706C"
      },
      "source": [
        "text['comment_text'] = text['comment_text'].map(lambda com : text_preprocess(com))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1G46ft37080"
      },
      "source": [
        "text.to_csv(\"preprocessed_text.csv\", index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY1dWEZ270_a"
      },
      "source": [
        "def remove_long(sent):\r\n",
        "    sentence = []\r\n",
        "    for word in sent.split():\r\n",
        "        if len(word)>15:\r\n",
        "            continue\r\n",
        "        else:\r\n",
        "            sentence.append(word)\r\n",
        "    return ' '.join(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeTMVwUc71CS"
      },
      "source": [
        "com = pd.read_csv('preprocessed_text.csv')\r\n",
        "com.dropna(inplace=True)\r\n",
        "com['comment_text'] = com['comment_text'].map(lambda com : remove_long(com))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDQK0POY78TU"
      },
      "source": [
        "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\r\n",
        "X = com['comment_text']\r\n",
        "y = com[categories]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ7Qmgr078V8"
      },
      "source": [
        "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoOlZ1xo71Gw"
      },
      "source": [
        "vect = TfidfVectorizer(use_idf=True, ngram_range=(1,1))\r\n",
        "tfidf = vect.fit(X_train)\r\n",
        "X_train = tfidf.transform(X_train)\r\n",
        "X_test = tfidf.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfQSeZU71J3"
      },
      "source": [
        "pickle.dump(tfidf, open('tfidf.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gNWRzii78Yv",
        "outputId": "cdf0ab2d-4957-4ac9-e990-2d3f556c5379"
      },
      "source": [
        "clf = XGBClassifier(n_estimators=150, n_jobs=-1)\r\n",
        "for category in categories:\r\n",
        "    print('{}: '.format(category))\r\n",
        "    clf.fit(X_train, y_train[category])\r\n",
        "    prediction = clf.predict(X_test)\r\n",
        "    print('F1 score is {}'.format(f1_score(y_test[category], prediction)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic: \n",
            "F1 score is 0.6192090395480226\n",
            "severe_toxic: \n",
            "F1 score is 0.22082018927444796\n",
            "obscene: \n",
            "F1 score is 0.7275641025641024\n",
            "threat: \n",
            "F1 score is 0.2261904761904762\n",
            "insult: \n",
            "F1 score is 0.5829619921363041\n",
            "identity_hate: \n",
            "F1 score is 0.36092715231788075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNsXzUHn78bV",
        "outputId": "545a6fa5-750e-4a91-e6fe-632d12e31fc0"
      },
      "source": [
        "clf = LogisticRegression(n_jobs=-1, class_weight='balanced')\r\n",
        "for category in categories:\r\n",
        "    print('{}: '.format(category))\r\n",
        "    clf.fit(X_train, y_train[category])\r\n",
        "    prediction = clf.predict(X_test)\r\n",
        "    print('F1 score is {}'.format(f1_score(y_test[category], prediction)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic: \n",
            "F1 score is 0.7433869648213799\n",
            "severe_toxic: \n",
            "F1 score is 0.41297935103244837\n",
            "obscene: \n",
            "F1 score is 0.7805040894675348\n",
            "threat: \n",
            "F1 score is 0.2696629213483146\n",
            "insult: \n",
            "F1 score is 0.6711047709621176\n",
            "identity_hate: \n",
            "F1 score is 0.36412749864937877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58pmVsSe78eR"
      },
      "source": [
        "clf = RandomForestClassifier(max_depth=None, n_estimators=70)\r\n",
        "for category in categories:\r\n",
        "    print('{}: '.format(category))\r\n",
        "    clf.fit(X_train, y_train[category])\r\n",
        "    prediction = clf.predict(X_test)\r\n",
        "    print('F1 score is {}'.format(f1_score(y_test[category], prediction)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE7Bjo8I8E5q",
        "outputId": "64608d70-b703-4305-e397-8ecae9b355ac"
      },
      "source": [
        "clf = clf = MultinomialNB()\r\n",
        "for category in categories:\r\n",
        "    print('{}: '.format(category))\r\n",
        "    clf.fit(X_train, y_train[category])\r\n",
        "    prediction = clf.predict(X_test)\r\n",
        "    print('F1 score is {}'.format(f1_score(y_test[category], prediction)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic: \n",
            "F1 score is 0.33812193412754027\n",
            "severe_toxic: \n",
            "F1 score is 0.0\n",
            "obscene: \n",
            "F1 score is 0.24088323854131816\n",
            "threat: \n",
            "F1 score is 0.014084507042253521\n",
            "insult: \n",
            "F1 score is 0.1203065134099617\n",
            "identity_hate: \n",
            "F1 score is 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj7qNfJTPrWc"
      },
      "source": [
        "# Creating models for each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ELbZSuxkfQz"
      },
      "source": [
        "model = LogisticRegression(n_jobs=-1, class_weight='balanced')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEaQ0jtaDtFG"
      },
      "source": [
        "model.fit(X_train, y_train[categories[0]])\r\n",
        "pickle.dump(model, open('toxic.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS3qtFq7DtRY"
      },
      "source": [
        "model.fit(X_train, y_train[categories[1]])\r\n",
        "pickle.dump(model, open('severe_toxic.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMvr6jafDtUB"
      },
      "source": [
        "model.fit(X_train, y_train[categories[2]])\r\n",
        "pickle.dump(model, open('obscene.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzdToVIP1CA"
      },
      "source": [
        "model.fit(X_train, y_train[categories[3]])\r\n",
        "pickle.dump(model, open('threat.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idBGzneKP1Ic"
      },
      "source": [
        "model.fit(X_train, y_train[categories[4]])\r\n",
        "pickle.dump(model, open('insult.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acL7dYOhP1PS"
      },
      "source": [
        "model.fit(X_train, y_train[categories[5]])\r\n",
        "pickle.dump(model, open('identity_hate.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0htmUvwZYsbm",
        "outputId": "6a3e3221-cb37-4296-f912-ed083616278d"
      },
      "source": [
        "pip install flask_ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask_ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjlz-Ylnb6tl",
        "outputId": "12f67946-c9b0-452f-e376-971d0a54881e"
      },
      "source": [
        "from flask import Flask, request, render_template\r\n",
        "from flask_ngrok import run_with_ngrok\r\n",
        "\r\n",
        "app = Flask(__name__)\r\n",
        "run_with_ngrok(app)\r\n",
        "\r\n",
        "def text_preprocess(sent):\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    stemmer = PorterStemmer() \r\n",
        "    \r\n",
        "    # convert all words to lowercase\r\n",
        "    sent = sent.lower()\r\n",
        "    \r\n",
        "    # expand contractions\r\n",
        "    expanded_words = []\r\n",
        "    for word in sent.split():\r\n",
        "        if len(word)>20:\r\n",
        "            continue\r\n",
        "        else:\r\n",
        "            expanded_words.append(contractions.fix(word))\r\n",
        "    sent = ' '.join(expanded_words) \r\n",
        "    \r\n",
        "    # remove html tags\r\n",
        "    sent = re.sub('{html}', \"\", sent)\r\n",
        "    \r\n",
        "    # remove http links and web site url\r\n",
        "    sent = re.sub(r\"http\\S+\", \"\", sent)\r\n",
        "    sent = re.sub(r\"www\\S+\", \"\", sent)\r\n",
        "    \r\n",
        "    # remove numbers\r\n",
        "    sent = re.sub('[0-9]+', '', sent)\r\n",
        "    \r\n",
        "    # remove words utc and image\r\n",
        "    sent = sent.replace('utc', '')\r\n",
        "    \r\n",
        "    # remove words with .jpg\r\n",
        "    sent = re.sub(r\"[a-zA-Z]*[0-9]*.jpg\", '', sent)\r\n",
        "    \r\n",
        "    # remove emails\r\n",
        "    sent = re.sub(r\"\\S*@\\S*\\s?\", '', sent)\r\n",
        "    \r\n",
        "    # tokenization\r\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    tokens = tokenizer.tokenize(sent.lower())\r\n",
        "    \r\n",
        "    # remove stop words\r\n",
        "    filtered_words = [w for w in tokens if w not in stopwords.words('english')]\r\n",
        "    \r\n",
        "    stem_words = [stemmer.stem(w) for w in filtered_words]\r\n",
        "    lemma_words = [lemmatizer.lemmatize(w) for w in stem_words]\r\n",
        "    \r\n",
        "    return \" \".join(lemma_words)\r\n",
        " \r\n",
        "with open('tfidf.pkl', \"rb\") as f:\r\n",
        "    tfidf = pickle.load(f)\r\n",
        "\r\n",
        "with open('toxic.pkl', \"rb\") as f:\r\n",
        "    toxic = pickle.load(f)\r\n",
        "with open('severe_toxic.pkl', \"rb\") as f:\r\n",
        "    severe_toxic = pickle.load(f)\r\n",
        "with open('obscene.pkl', \"rb\") as f:\r\n",
        "    obscene = pickle.load(f)\r\n",
        "with open('threat.pkl', \"rb\") as f:\r\n",
        "    threat = pickle.load(f)\r\n",
        "with open('insult.pkl', \"rb\") as f:\r\n",
        "    insult = pickle.load(f)\r\n",
        "with open('identity_hate.pkl', \"rb\") as f:\r\n",
        "    identity_hate = pickle.load(f)\r\n",
        "\r\n",
        "\r\n",
        "@app.route('/')\r\n",
        "def home():\r\n",
        "   return render_template('index.html')\r\n",
        "            \r\n",
        "@app.route('/predict', methods=['POST'])\r\n",
        "def predict():\r\n",
        "   comment = [text_preprocess(str(x)) for x in request.form.values()]\r\n",
        "   data = tfidf.transform(comment)\r\n",
        "   toxic_pred = toxic.predict_proba(data)\r\n",
        "   severe_toxic_pred = severe_toxic.predict_proba(data)\r\n",
        "   obscene_pred = obscene.predict_proba(data)\r\n",
        "   threat_pred = threat.predict_proba(data)\r\n",
        "   insult_pred = insult.predict_proba(data)\r\n",
        "   identity_hate_pred = identity_hate.predict_proba(data)\r\n",
        "   \r\n",
        "   #labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\r\n",
        "   #results = [toxic_pred[0][1], severe_toxic_pred[0][1], obscene_pred[0][1], threat_pred[0][1], insult_pred[0][1], identity_hate_pred[0][1]]\r\n",
        "\r\n",
        "   return render_template('index.html', prediction_text=\"Probabilities are: toxic -> {0}, severe_toxic -> {1}, obscene -> {2}, threat -> {3}, insult -> {4}, identity_hate -> {5}\".format(toxic_pred[0][1], \r\n",
        "                                     severe_toxic_pred[0][1],\r\n",
        "                                     obscene_pred[0][1],\r\n",
        "                                     threat_pred[0][1],\r\n",
        "                                     insult_pred[0][1],\r\n",
        "                                     identity_hate_pred[0][1]))\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "   app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://d81b11e4b716.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/Dec/2020 00:42:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/Dec/2020 00:42:21] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [12/Dec/2020 00:42:26] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}