{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedded layers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWUbThYIo8DV",
        "outputId": "2cf4bea8-604c-4076-9dee-c6808dcd7fce"
      },
      "source": [
        "import sys  \r\n",
        "!{sys.executable} -m pip install contractions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/ad/d1c685967945a04f8596128b15a1ab56c51488f53312e953341af6ff22d1/contractions-0.0.43-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.0MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 10.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81699 sha256=4308e1e703bc4d5fbfe39e913601b54b8240cfd97ff388fe310cbd1417c705cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.43 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQlECP3v96gF",
        "outputId": "1621070f-baa0-413f-8a42-28d4576fb0a8"
      },
      "source": [
        "import sys\r\n",
        "print(sys.getrecursionlimit())\r\n",
        "sys.setrecursionlimit(5000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_knn_lNK-MCt",
        "outputId": "88f49562-e384-4e24-9b2b-b58b1a496df8"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('popular')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_LWJRcX-MGG"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.text import one_hot\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers.core import Activation, Dropout, Dense\r\n",
        "from keras.layers import Flatten, LSTM\r\n",
        "from keras.layers import GlobalMaxPooling1D\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers.merge import Concatenate\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXGTVB-j-MI7"
      },
      "source": [
        "def remove_long(sent):\r\n",
        "    sentence = []\r\n",
        "    for word in sent.split():\r\n",
        "        if len(word)>15:\r\n",
        "            continue\r\n",
        "        else:\r\n",
        "            sentence.append(word)\r\n",
        "    return ' '.join(sentence)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71iofRcy-MME"
      },
      "source": [
        "toxic_comments = pd.read_csv('preprocessed_text.csv')\r\n",
        "toxic_comments.dropna(inplace=True)\r\n",
        "toxic_comments['comment_text'] = toxic_comments['comment_text'].map(lambda com : remove_long(com))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8HHKhw-MOZ"
      },
      "source": [
        "labels = toxic_comments[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd3_pFPY-MRL"
      },
      "source": [
        "X_train , X_test, y_train, y_test= train_test_split(toxic_comments['comment_text'], labels, test_size= 0.1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ8j89cU-MTi",
        "outputId": "058c3918-1791-4141-f211-f6a1930d6495"
      },
      "source": [
        "vect = Tokenizer()\r\n",
        "vect.fit_on_texts(X_train)\r\n",
        "vocab_size = len(vect.word_index) + 1\r\n",
        "print(vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOlZWOYL-ckd",
        "outputId": "fa2d94eb-9848-4d5b-fb2d-816f27b08035"
      },
      "source": [
        "encoded_docs_train = vect.texts_to_sequences(X_train)\r\n",
        "max_length = vocab_size\r\n",
        "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=1000, padding='post')\r\n",
        "print(padded_docs_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5253    59    25 ...     0     0     0]\n",
            " [  554    25    57 ...     0     0     0]\n",
            " [    6     7   134 ...     0     0     0]\n",
            " ...\n",
            " [   97 33069     2 ...     0     0     0]\n",
            " [  144   114   231 ...     0     0     0]\n",
            " [ 1715    70    16 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqqPj8e_-cnS"
      },
      "source": [
        "encoded_docs_test =  vect.texts_to_sequences(X_test)\r\n",
        "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=1000, padding='post')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUDhbCkL-cp_",
        "outputId": "f229ea8c-6278-4200-bc81-c0c964fbfd9a"
      },
      "source": [
        "model = Sequential()\r\n",
        "# Configuring the parameters\r\n",
        "model.add(Embedding(vocab_size, output_dim=50, input_length=1000))\r\n",
        "model.add(LSTM(128, return_sequences=True))  \r\n",
        "# Adding a dropout layer\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(LSTM(64))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "# Adding a dense output layer with sigmoid activation\r\n",
        "model.add(Dense(6, activation='sigmoid'))\r\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 50)          6071200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 1000, 128)         91648     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1000, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 6,212,646\n",
            "Trainable params: 6,212,646\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjfQcJk6-csq",
        "outputId": "87114d63-196f-4be6-fad0-f5c5ca10c4eb"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy')\r\n",
        "history = model.fit(padded_docs_train, y_train,\r\n",
        "                    class_weight=None,\r\n",
        "                    epochs=3,\r\n",
        "                    batch_size=32,\r\n",
        "                    validation_split=0.1,\r\n",
        "                    callbacks=[])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "4038/4038 [==============================] - 7112s 2s/step - loss: 0.1478 - val_loss: 0.1388\n",
            "Epoch 2/3\n",
            "4038/4038 [==============================] - 7783s 2s/step - loss: 0.1436 - val_loss: 0.1384\n",
            "Epoch 3/3\n",
            " 925/4038 [=====>........................] - ETA: 1:37:45 - loss: 0.1430"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCG2ZgKU-mlL"
      },
      "source": [
        "score = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O96fnAZE_11b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}